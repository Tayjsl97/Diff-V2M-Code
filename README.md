# Diff-V2M
This is the official implementation of **Diff-V2M (AAAI'26)**, which is a hierarchical diffusion model with explicit rhythmic modeling and multi-view feature conditioning, achieving state-of-the-art results in video-to-music generation.
- [Paper link](https://arxiv.org/abs/2312.10307)
- Check our [demo page](https://tayjsl97.github.io/Diff-V2M-Demo/) and listen!<br>

<img src="img/model.jpg" width="770" height="300" alt="model"/>

## Reference
If you find the code useful for your research, please consider citing
```bib
@inproceedings{ji2026diff,
  title={Diff-V2M: A Hierarchical Conditional Diffusion Model with Explicit Rhythmic Modeling for Video-to-Music Generation},
  author={Ji, Shulei and Wang, Zihao and Yu, Jiaxing and Yang, Xiangyuan and Li, Shuyu and Wu, Songruoyao and Zhang, Kejun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2026}
}
```
## Acknowledgments
We appreciate [stable audio open](https://github.com/Stability-AI/stable-audio-tools) for providing the reference codes of audio generation models.

